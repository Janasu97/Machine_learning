{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = pd.read_csv(\"iris.data\", names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"])\n",
    "#splitting dataset into X (iris_data) and y (iris_types)\n",
    "iris_data = iris[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]]\n",
    "iris_types = iris[[\"class\"]]\n",
    "#preparing data\n",
    "#changing the types of iris into numbers [0, 1, 2]\n",
    "#normalizing iris_data so the values will be between (0,1)\n",
    "#concatenating prepared data with indexes\n",
    "iris_types = iris_types.replace([\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"], [0, 1, 2])\n",
    "normalized_iris_data = (iris_data-iris_data.min())/(iris_data.max()-iris_data.min())\n",
    "preprocessed_data = pd.concat([normalized_iris_data, iris_types], axis=1)\n",
    "\n",
    "#Shuffling and splitting data for model training and testing in n (ratio of data distribution)\n",
    "preprocessed_data = preprocessed_data.sample(frac=1).reset_index(drop=True)\n",
    "n = 0.3\n",
    "#for 0.3 it will be 105 elements for training and 45 for testing\n",
    "data_ratio = int(len(preprocessed_data)*n)\n",
    "test_data = preprocessed_data.iloc[:data_ratio]\n",
    "train_data = preprocessed_data.iloc[data_ratio:]\n",
    "#separating training data into X and one hot encoded y\n",
    "#X preparing\n",
    "X_train = train_data.values[:, :4]\n",
    "X_test = test_data.values[:, :4]\n",
    "#y preparing\n",
    "one_hot_encoding = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "y_train = np.array([one_hot_encoding[int(x)] for x in train_data.values[:, 4]])\n",
    "y_test =  np.array([one_hot_encoding[int(x)] for x in test_data.values[:, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hid1/hid2 stands for first/second hidden layer\n",
    "\n",
    "#Setting number of neurons in hidden layers\n",
    "#Setting learning rate for weights and biases and number of epochs\n",
    "neurons_number_hid1 = 20\n",
    "neurons_number_hid2 = 20\n",
    "learning_rate = 0.1\n",
    "\n",
    "# First randomized weights for all layers of NN (min val, max val, size)\n",
    "w1 = np.random.uniform(-1, 1, (len(X_train[0]), neurons_number_hid1)) \n",
    "w2 = np.random.uniform(-1, 1, (neurons_number_hid1, neurons_number_hid2))\n",
    "w3 = np.random.uniform(-1, 1, (neurons_number_hid2, len(y_train[0])))\n",
    "#First biases for hidden layers and output\n",
    "bias_hid1 = np.zeros((neurons_number_hid1))\n",
    "bias_hid2 = np.zeros((neurons_number_hid2))\n",
    "output_layer_bias = np.zeros((len(y_train[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining activation functions\n",
    "#RELu activation function setting all negative functions to 0\n",
    "# if x > 0 => x, else 0\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "#Softmax activation function\n",
    "def softmax(output_array):\n",
    "    exp = np.exp(output_array)\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a model of NN with forward and back propagation\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #creating of model with input layer, 2 hidden layers, output layer\n",
    "    input_layer = np.dot(X_train, w1) + bias_hid1\n",
    "    hid1_layer_activation = relu(input_layer)\n",
    "    hid2_layer = np.dot(hid1_layer_activation, w2) + bias_hid2\n",
    "    hid2_layer_activation = relu(hid2_layer)\n",
    "    output_layer = np.dot(hid2_layer_activation, w3) + output_layer_bias\n",
    "    output_layer_activation = softmax(output_layer) \n",
    "    \n",
    "    # Error calculation in hidden layers\n",
    "    output_error = (output_layer_activation - y_train) / len(output_layer_activation)\n",
    "    error_hid2 = np.dot(output_error, w3.T)\n",
    "    error_hid1 = np.dot(error_hid2, w2.T)\n",
    "\n",
    "    # Back propagation to adjust the weight and bias values \n",
    "    grad_output_w = np.dot(hid2_layer_activation.T, output_error)\n",
    "    grad_output_bias = np.sum(output_error, axis=0, keepdims=True)\n",
    "    w3 = w3 - learning_rate * grad_output_w\n",
    "    output_layer_bias = output_layer_bias - learning_rate * grad_output_bias\n",
    "    \n",
    "    grad_h2_w = np.dot(hid1_layer_activation.T, error_hid2)\n",
    "    grad_h2_bias = np.sum(error_hid2, axis=0, keepdims=True)\n",
    "    w2 = w2 - learning_rate * grad_h2_w\n",
    "    bias_hid2 = bias_hid2 - learning_rate * grad_h2_bias\n",
    "    \n",
    "    grad_h1_w = np.dot(X_train.T, error_hid1)\n",
    "    grad_h1_bias = np.sum(error_hid1, axis=0, keepdims=True)\n",
    "    w1 = w1 - learning_rate * grad_h1_w\n",
    "    bias_hid1 = bias_hid1 - learning_rate * grad_h1_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 93.33%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of test run\n",
    "def accuracy(predicted, actual):\n",
    "    is_correct = np.argmax(predicted, axis=1) == np.argmax(actual, axis=1)\n",
    "    correct_predictions = np.sum(is_correct)\n",
    "    accuracy = correct_predictions / len(predicted) * 100.0\n",
    "    return accuracy\n",
    "\n",
    "input_layer = np.dot(X_test, w1) + bias_hid1\n",
    "hid1_layer_activation = relu(input_layer)\n",
    "hid2_layer = np.dot(hid1_layer_activation, w2) + bias_hid2\n",
    "hid2_layer_activation = relu(hid2_layer)\n",
    "output_layer = np.dot(hid2_layer_activation, w3) + output_layer_bias\n",
    "prediction = softmax(output_layer)\n",
    "\n",
    "print(f'Test accuracy: {round(accuracy(prediction, y_test), 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For index number: \"26\" of test data the model was mistaken.\n",
      "Actual type: 1 Predicted type: 2\n",
      "For index number: \"31\" of test data the model was mistaken.\n",
      "Actual type: 2 Predicted type: 1\n",
      "For index number: \"39\" of test data the model was mistaken.\n",
      "Actual type: 2 Predicted type: 1\n",
      "Model was mistaken 0 times for setosa, 1 times for vergicolor\n",
      "and 2 times for virginica.\n"
     ]
    }
   ],
   "source": [
    "setosa_num = 0\n",
    "vergicolor_num = 0\n",
    "virginica_num = 0\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    if np.argmax(y_test[i]) != np.argmax(prediction[i]):\n",
    "        print(f'For index number: \"{i}\" of test data the model was mistaken.')\n",
    "        print(f'Actual type: {np.argmax(y_test[i])} Predicted type: {np.argmax(prediction[i])}')\n",
    "        if np.argmax(y_test[i]) == 0:\n",
    "            setosa_num = setosa_num + 1\n",
    "        elif np.argmax(y_test[i]) == 1:\n",
    "            vergicolor_num = vergicolor_num + 1\n",
    "        elif np.argmax(y_test[i]) == 2:\n",
    "            virginica_num = virginica_num + 1\n",
    "print(f'Model was mistaken {setosa_num} times for setosa, {vergicolor_num} times for vergicolor')\n",
    "print(f'and {virginica_num} times for virginica.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records for flower type in training dataset:\n",
      "setosa = 13\n",
      "vergicolor = 16\n",
      "virginica = 16\n"
     ]
    }
   ],
   "source": [
    "#checking of flower types data distribution\n",
    "iris_type = test_data.values[:, 4]\n",
    "setosa = 0\n",
    "vergicolor = 0\n",
    "virginica = 0\n",
    "for x in range(len(test_data)):\n",
    "    if iris_type[x] == 0:\n",
    "        setosa += 1\n",
    "    elif iris_type[x] == 1:\n",
    "        vergicolor += 1\n",
    "    else:\n",
    "        virginica += 1\n",
    "    x += 1\n",
    "    \n",
    "print(f'Number of records for flower type in training dataset:')\n",
    "print(f'setosa = {setosa}')\n",
    "print(f'vergicolor = {vergicolor}')\n",
    "print(f'virginica = {virginica}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
